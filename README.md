
# Wiki Search Engine
![Java](https://img.shields.io/badge/Java-17-brightgreen)  ![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.4.5-green)  ![Maven](https://img.shields.io/badge/Maven-3.x-blue)  ![Jsoup](https://img.shields.io/badge/Jsoup-1.18.3-orange)  ![OpenNLP](https://img.shields.io/badge/OpenNLP-2.3.1-lightblue)  ![HTML](https://img.shields.io/badge/HTML-5-red)  ![CSS](https://img.shields.io/badge/CSS-3-blue)  ![JavaScript](https://img.shields.io/badge/JavaScript-ES6-yellow)
## ğŸš€ Overview

**Wiki Search Engine** is a powerful search application designed for efficiently indexing and searching Wikipedia content. This application utilizes modern Java technologies along with natural language processing to provide fast, accurate, and relevant search results for wiki content.

## âœ¨ Features

- **Advanced Search Functionality** â€“ Quickly search and navigate through wiki content with precision  
- **Tokenization & Stemming** â€“ Using Apache OpenNLP to tokenize and stem terms for more accurate search matching  
- **RESTful API** â€“ Spring Boot backend providing robust search endpoints  
- **HTML Parsing** â€“ Jsoup integration for effective content extraction from wiki pages  
- **Front-End Interface** â€“ Desktop-focused UI built with HTML, CSS, and JavaScript, featuring a search input box and a paginated results list each result links directly to the original Wikipedia article 
- **Customizable Crawling** â€“ Define custom seed URLs and crawl limits via command line 

## ğŸ“‚ Project Structure

```plaintext
wiki-search-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ java/
â”‚   â”‚   â”‚   â””â”€â”€ com/ir/wikisearchengine/
â”‚   â”‚   â”‚       â”œâ”€â”€ controller/      # REST controllers to handle API requests
â”‚   â”‚   â”‚       â”œâ”€â”€ crawler/         # Logic for crawling and fetching wiki pages
â”‚   â”‚   â”‚       â”œâ”€â”€ dto/             # Data transfer objects used for request and response models
â”‚   â”‚   â”‚       â”œâ”€â”€ indexing/        # Core logic for indexing and storing wiki data
â”‚   â”‚   â”‚       â”œâ”€â”€ model/           # Data models representing wiki content and related entities
â”‚   â”‚   â”‚       â”œâ”€â”€ Service/         # Business logic and service layer components
â”‚   â”‚   â”‚       â”œâ”€â”€ startup/         # Initialization logic that runs on application startup
â”‚   â”‚   â”‚       â””â”€â”€ WikiSearchEngineApplication.java  # Main application class and entry point
â”‚   â”‚   â””â”€â”€ resources/
â”‚   â”‚       â”œâ”€â”€ static/              # Static web resources (CSS, JS)
â”‚   â”‚       â”œâ”€â”€ templates/           # HTML templates for rendering views
â”‚   â”‚       â””â”€â”€ application.properties # Application configuration
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/
â”‚           â””â”€â”€ com/ir/wikisearchengine/
â”‚               â””â”€â”€ WikiSearchEngineApplicationTests.java  # Unit and integration tests
â”œâ”€â”€ target/                          # Compiled output and packaged artifacts
â”œâ”€â”€ .gitignore                       # Git ignore file
â”œâ”€â”€ pom.xml                          # Maven dependencies and project configuration
â””â”€â”€ README.md                        # Project documentation
```

## ğŸ› ï¸ Tech Stack

- **Backend**: Java 17, Spring Boot 3.4.5  
- **NLP**: Apache OpenNLP 2.3.1  
- **HTML Parsing**: JSoup 1.18.3  
- **Development Tools**: Lombok  
- **Build Tool**: Maven  

## âš¡ Installation and Setup

### Prerequisites

- Java 17 or higher  
- Maven 3.x  

### Steps

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Hemdan47/wiki-search-engine.git
   cd wiki-search-engine
   ```

2. **Build the project**:
   ```bash
   mvn clean install
   ```

3. **Run the application with optional crawl parameters**:

   You can pass seed URLs and the max number of pages to crawl via command line arguments. If no arguments are given, the application defaults to:
   - Seed: `https://en.wikipedia.org/wiki/Main_Page`
   - Max pages: `100`

   **Example usage**:
   ```bash
   ./mvnw spring-boot:run '-Dspring-boot.run.arguments=https://en.wikipedia.org/wiki/List_of_pharaohs https://en.wikipedia.org/wiki/Pharaoh 10'
   ```

4. **Access the application**:
   - Open a web browser and navigate to `http://localhost:8080`

## ğŸ”§ How It Works

### ğŸ•¸ï¸ Crawling

The application begins by reading a set of **seed URLs** and a **maximum number of pages** from the command-line arguments. If no arguments are provided, it defaults to crawling from `https://en.wikipedia.org/wiki/Main_Page` and limits itself to 100 pages.

- A queue (`LinkedList<String>`) is used to manage URLs to be visited.
- A `HashSet<String>` tracks visited URLs to avoid duplicates.
- Only internal Wikipedia links are followed.
- Jsoup is used to fetch and parse HTML content.

---

### ğŸ“š Indexing

An **inverted index** is created to map each word to the documents in which it appears.

Steps:
- Extract and normalize text: tokenize using regex (split("\\W+")), lowercase, and apply OpenNLP stemming.
- Build a `Map<String, List<Posting>>` where each `Posting` contains:
  - `documentId`
  - `term frequency (TF)`

This allows fast lookup of terms and efficient search processing.

---

### ğŸ“ TF-IDF Calculation

Each document is converted into a TF-IDF vector.

- **TF**:  
  ```java
  weight = 1 + log10(tf)
  ```

- **IDF**:  
  ```java
  idf = log10(N / df)
  ```

- **TF-IDF**:  
  ```java
  tfIdf = tfWeight * idf
  ```

Where `N` is the total number of documents and `df` is how many documents contain the term.

---


### ğŸ§  Query Processing and Cosine Similarity

- User inputs a query string.
- Query is tokenized and TF-IDF weighted like documents.
- Cosine similarity is calculated:
  ```java
  cosineSimilarity = dot(d, q) / (norm(d) * norm(q))
  ```
- Results are **ranked by similarity score** and **returned in pages** using a built-in pagination system.
---

### ğŸ“– Usage

#### Web Interface
- Built using **HTML, CSS, and JavaScript** for a clean experience.
- Enter your search query in the search box.
- Results are paginated and ranked by relevances.
- Click a result to view the full wiki content.

#### REST API
- `GET /search?query={searchTerm}&page={pageNumber}` â€“ Perform paginated search across indexed documents.

---

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).
